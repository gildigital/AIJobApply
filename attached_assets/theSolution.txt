Based on the workable-scraper.ts code and your description, the core issue likely lies in how and where the individual job detail fetches are happening and how the timeouts are being applied.

Here's a breakdown of the likely problem and proposed solutions:

Diagnosis:

Timeout Misapplication: The Promise.race timeout you implemented inside scrapeJobsFromSearchUrl is applied to the extractJobInfoFromPage function call. However, within that loop, extractJobInfoFromPage is called with the search results page HTML (extractJobInfoFromPage(jobLink, html)). This function, when given HTML, primarily parses that existing HTML rather than making a new network request to jobLink. Parsing local HTML is very fast and unlikely to time out. The timeout isn't actually wrapping the slow network request for the individual job detail page.
Sequential Blocking Fetch: The actual fetching of the full job details (which seems to happen via the fetchJobDetails function calling your /api/workable/direct-fetch endpoint, or perhaps elsewhere in the getWorkableJobsForUser orchestration) is likely happening sequentially after the initial batch of links is gathered. Crucially, this actual network fetch doesn't appear to be wrapped by the Promise.race timeout you added. Therefore, when one of these fetches hangs, it blocks the entire process because the code waits indefinitely for it to complete before moving to the next job.
Lack of True Cancellation: Even if a timeout were correctly applied using Promise.race, it doesn't cancel the underlying network request. The stalled fetch would continue running in the background, consuming resources.
Proposed Solutions & Architectural Improvements:

We need to ensure that the actual network request for each job detail page is handled concurrently and has a cancellable timeout.

Concurrent Fetching with Promise.allSettled: Instead of fetching job details one by one sequentially, fetch them in parallel batches. Promise.allSettled is ideal because it waits for all promises in an iterable to settle (either fulfilled or rejected), meaning one failed/timed-out request won't stop the others.
Cancellable Timeouts with AbortController: Implement timeouts for each individual fetch request using the standard AbortController API. This allows you to truly cancel the network request if it takes too long.
Refactor Fetching Logic: Modify the process to fetch details concurrently with proper timeouts. This might involve changing scrapeJobsFromSearchUrl or the logic within getWorkableJobsForUser that orchestrates the fetching.
(Optional but Recommended) Background Job Queue: For a truly robust and scalable solution, especially if scraping can take time, move the entire scraping process for a user search into a background job queue (e.g., BullMQ, Redis Queue).
API: /find-jobs endpoint adds a job to the queue and immediately returns a jobId.
Client: Polls a /job-status/:jobId endpoint or uses WebSockets to get progress updates and final results.
Worker: A separate process picks up jobs from the queue, performs the scraping (with concurrency, timeouts, retries), and updates the job status. This prevents blocking API requests and handles failures gracefully.
Code Implementation Sketch (Focusing on AbortController and Promise.allSettled):

Let's modify the part where job details are likely fetched. Assuming fetchJobDetails is the function responsible for getting the full details (and thus the one prone to hanging):

```
import { UserProfile, JobPreferences } from "@shared/schema";
import { storage } from "../storage";
import crypto from "crypto";

// ... (Keep existing interfaces: WorkableSearchParams, WorkableJob, JobListing, SearchState) ...

export class WorkableScraper {
  // ... (Keep BASE_URL, searchStates, initializeSearchState, saveSearchState, getSearchState, cleanupOldSearchStates, buildSearchUrl, generateSearchUrls, isValidWorkableJobUrl, isValidWorkableApplicationUrl, introspectJobForm) ...

  /**
   * Fetch Workable job details with a cancellable timeout.
   * @param url The URL of the job detail page.
   * @param timeoutMs Timeout duration in milliseconds.
   * @returns WorkableJob or null if fetch fails or times out.
   */
  async fetchJobDetailsWithTimeout(url: string, timeoutMs: number = 5000): Promise<WorkableJob | null> {
    // AbortController allows cancelling the fetch request
    const controller = new AbortController();
    const signal = controller.signal;

    // Set a timer to abort the request after timeoutMs
    const timeoutId = setTimeout(() => {
      console.log(`Timing out fetch for ${url} after ${timeoutMs}ms`);
      controller.abort();
    }, timeoutMs);

    try {
      console.log(`Fetching job details for ${url} with timeout ${timeoutMs}ms`);
      // Use your direct fetch API endpoint
      // *** IMPORTANT: Ensure this endpoint itself doesn't have excessively long internal timeouts ***
      const apiUrl = `http://localhost:5000/api/workable/direct-fetch?url=${encodeURIComponent(url)}`;

      const response = await fetch(apiUrl, {
        signal, // Pass the AbortSignal to fetch
        headers: {
           // Add any necessary headers, e.g., User-Agent
          'User-Agent': 'Mozilla/5.0 (...', // Add a realistic User-Agent
          'Accept': 'application/json', // Assuming your API returns JSON
        }
      });

      // Clear the timeout timer if the fetch completes or fails before the timeout
      clearTimeout(timeoutId);

      if (!response.ok) {
        // Handle non-2xx responses (e.g., 404, 500)
        console.error(`Failed to fetch job details (${url}): ${response.status} ${response.statusText}`);
        return null;
      }

      const data = await response.json();

      // Validate the structure of data.job before returning
      if (data && data.job && typeof data.job.title === 'string') {
         // Add source and default status/appliedAt if missing from API response
         return {
            source: 'workable',
            status: 'found',
            appliedAt: null,
            ...data.job // Spread the properties from your API response
         } as WorkableJob;
      } else {
        console.error(`Invalid job data received from direct-fetch API for ${url}`);
        return null;
      }

    } catch (error: any) {
      // Clear the timeout timer in case of other errors
      clearTimeout(timeoutId);

      if (error.name === 'AbortError') {
        // This specific error is thrown when controller.abort() is called
        console.warn(`Fetch aborted for ${url} due to timeout.`);
      } else {
        // Log other potential errors (network issues, JSON parsing errors, etc.)
        console.error(`Error fetching job details for ${url}:`, error.message);
      }
      return null;
    }
  }


  /**
   * Scrape job listings from a Workable search URL, fetching details concurrently.
   * @param searchUrl The URL of the search results page
   * @param state The current search state (to avoid duplicates)
   * @param jobDetailTimeoutMs Timeout for fetching each individual job detail page.
   * @returns Array of job listings found on the page.
   */
  async scrapeJobsFromSearchUrl(
    searchUrl: string,
    state: SearchState, // Pass state to check for duplicates
    jobDetailTimeoutMs: number = 5000 // e.g., 5 seconds per job
  ): Promise<JobListing[]> {
    try {
      console.log(`Fetching job listings from: ${searchUrl}`);
      // ... (keep headers and fetch logic for the search results page itself) ...
       const headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Referer': 'https://jobs.workable.com/'
      };
      const response = await fetch(searchUrl, { headers }); // Timeout for search page itself? Add signal here too if needed.

      if (!response.ok) {
        console.error(`Failed to fetch search results (${searchUrl}): ${response.statusText}, Status: ${response.status}`);
        return [];
      }
      const html = await response.text();
      // ... (keep logic for extracting jobLinks using Regex, JSON-LD etc.) ...
       const jobLinks: string[] = [];
       // Add your link extraction logic here (Regex, JSON-LD, etc.)
       // Example using Regex:
       const jobUrlRegex = /https:\/\/jobs\.workable\.com\/view\/[A-Za-z0-9]+/g;
       let match;
       while ((match = jobUrlRegex.exec(html)) !== null) {
         jobLinks.push(match[0]);
       }
       console.log(`Extracted ${jobLinks.length} potential job links from ${searchUrl}`);


      // Deduplicate and filter out already processed jobs
      const uniqueLinks = Array.from(new Set(jobLinks));
      const newLinks = uniqueLinks.filter(link => {
          // Extract potential ID or use full URL for uniqueness check
          const potentialId = link.split('/').pop();
          return potentialId && !state.jobIds.has(potentialId); // Check against jobIds in state
      });


      // Limit the number of *new* jobs to fetch details for concurrently per search page
      const MAX_JOBS_PER_PAGE = 10; // Adjust as needed
      const linksToFetchDetails = newLinks.slice(0, MAX_JOBS_PER_PAGE);

      if (linksToFetchDetails.length === 0) {
          console.log(`No new job links found on ${searchUrl} to fetch details for.`);
          return [];
      }

      console.log(`Attempting to fetch details for ${linksToFetchDetails.length} new job links concurrently from ${searchUrl}...`);

      // Create an array of promises, each fetching details for one job with timeout
      const detailFetchPromises = linksToFetchDetails.map(jobLink =>
        this.fetchJobDetailsWithTimeout(jobLink, jobDetailTimeoutMs)
          .then(jobDetail => ({ link: jobLink, detail: jobDetail })) // Keep link for context
      );

      // Use Promise.allSettled to wait for all fetches to complete or fail/timeout
      const results = await Promise.allSettled(detailFetchPromises);

      const jobListings: JobListing[] = [];
      results.forEach(result => {
        if (result.status === 'fulfilled' && result.value.detail) {
          // Successfully fetched job details
          const jobDetail = result.value.detail;
          const jobLink = result.value.link;

          // Add to jobIds set in state to prevent re-processing
           const potentialId = jobLink.split('/').pop();
           if(potentialId) state.jobIds.add(potentialId); // Add successfully processed job ID

          // Map WorkableJob to JobListing format
          jobListings.push({
            jobTitle: jobDetail.title,
            company: jobDetail.company,
            description: jobDetail.description, // Or a snippet
            applyUrl: jobDetail.url, // Use the URL from the detail if available, else jobLink
            location: jobDetail.location,
            source: 'workable',
            // externalJobId: jobLink.split('/').pop(), // Extract ID if needed
            // matchScore: this.calculateInitialMatchScore(jobDetail) // Calculate score
          });
        } else if (result.status === 'rejected') {
          // Log errors from fetchJobDetailsWithTimeout (already logged internally)
          console.error(`Failed to process job link (reason: ${result.reason})`);
        } else if (result.status === 'fulfilled' && !result.value.detail) {
           // Fetch completed but returned null (e.g., timeout, non-ok status, invalid data)
           console.warn(`Fetching completed but no valid details returned for job link: ${result.value.link}`);
        }
      });

       console.log(`Successfully fetched details for ${jobListings.length} jobs from ${searchUrl}.`);
       state.totalJobsFound += jobListings.length; // Update total count in state

      return jobListings;

    } catch (error) {
      console.error(`Error scraping jobs from search URL ${searchUrl}:`, error);
      return [];
    }
  }

  // Remove the old extractJobInfoFromPage function as its role is now handled
  // by fetchJobDetailsWithTimeout and the parsing within scrapeJobsFromSearchUrl

  /**
   * Main function to get jobs for a user, handling pagination and state.
   */
  async getWorkableJobsForUser(
    userId: number,
    progressCallback?: (progress: { current: number; total: number; status: string; jobs?: JobListing[], continueToken?: string | null }) => void,
    options: {
      pageSize?: number; // How many jobs to return per call
      maxInitialJobs?: number; // Target for the first call (might be less)
      searchDepth?: number; // Max pages per job title search (e.g., 3)
      continueToken?: string; // Token for pagination
      jobDetailTimeoutMs?: number; // Timeout for individual job fetches
    } = {}
  ): Promise<{ jobs: JobListing[]; continueToken: string | null }> {

    const {
        pageSize = 10,
        maxInitialJobs = 15, // Aim for this many on the first run
        searchDepth = 3,     // Fetch first 3 pages per search query initially
        continueToken,
        jobDetailTimeoutMs = 5000 // Default 5 sec timeout per job detail fetch
    } = options;

    let state: SearchState;

    // Retrieve state if continueToken is provided, otherwise initialize
    if (continueToken) {
      const retrievedState = await this.getSearchState(continueToken);
      if (!retrievedState) {
        console.error("Invalid or expired continueToken");
        // Optionally re-initialize or return error
         return { jobs: [], continueToken: null }; // Or throw error
      }
      state = retrievedState;
      console.log(`Resuming search with token. Current index: ${state.currentUrlIndex}/${state.searchUrls.length}`);
    } else {
      console.log("Initializing new search state.");
      const userProfile = await storage.getUserProfile(userId); // Fetch user profile
      state = this.initializeSearchState(userProfile, searchDepth);
    }

    const jobsFoundThisRun: JobListing[] = [];
    const targetJobs = continueToken ? pageSize : maxInitialJobs; // Different target for initial vs subsequent calls

    // Loop through search URLs until enough jobs are found or URLs run out
    while (state.currentUrlIndex < state.searchUrls.length && jobsFoundThisRun.length < targetJobs) {
      const currentSearchUrl = state.searchUrls[state.currentUrlIndex];
      console.log(`Processing search URL ${state.currentUrlIndex + 1}/${state.searchUrls.length}: ${currentSearchUrl}`);

      if (progressCallback) {
        progressCallback({
          current: state.currentUrlIndex + 1,
          total: state.searchUrls.length,
          status: `Scraping: ${currentSearchUrl}`,
           jobs: jobsFoundThisRun // Send intermediate results
        });
      }

      // Scrape jobs from the current URL, fetch details concurrently with timeout
      const jobsFromUrl = await this.scrapeJobsFromSearchUrl(currentSearchUrl, state, jobDetailTimeoutMs);
      jobsFoundThisRun.push(...jobsFromUrl);

      // Mark URL as processed and move to the next
      state.processedUrls.push(currentSearchUrl);
      state.currentUrlIndex++;

      console.log(`Found ${jobsFromUrl.length} new jobs from URL. Total this run: ${jobsFoundThisRun.length}. Overall total: ${state.totalJobsFound}`);
    }

    // Determine if there are more URLs to process
    const hasMore = state.currentUrlIndex < state.searchUrls.length;
    let nextContinueToken: string | null = null;

    if (hasMore) {
      // Save the updated state and get a new token for the next page
      nextContinueToken = await this.saveSearchState(state);
      console.log(`Search incomplete. Returning ${jobsFoundThisRun.length} jobs. Next token: ${nextContinueToken}`);
    } else {
      console.log(`Search complete. All ${state.searchUrls.length} URLs processed. Total jobs found: ${state.totalJobsFound}.`);
       // Clean up state if search is finished? Or let it expire naturally.
       // this.searchStates.delete(continueToken); // If using token from input
    }

     if (progressCallback) {
        progressCallback({
          current: state.currentUrlIndex,
          total: state.searchUrls.length,
          status: hasMore ? `Finished page, more available.` : `Search complete.`,
          jobs: jobsFoundThisRun,
          continueToken: nextContinueToken
        });
      }


    // Return the jobs found in this specific run and the token for the next run
    return {
      jobs: jobsFoundThisRun.slice(0, pageSize), // Ensure we don't return more than pageSize
      continueToken: nextContinueToken
    };
  }

   // ... (keep getDefaultWorkableJobs if needed) ...
    /**
   * Provides a default set of Workable jobs for testing or demo purposes
   * Ensures the URLs are valid Workable application URLs
   */
  getDefaultWorkableJobs(): JobListing[] {
    // Example default jobs - replace with actual relevant examples if needed
    const defaultWorkableJobs: JobListing[] = [
      {
        jobTitle: "Software Engineer",
        company: "Example Tech Inc.",
        description: "Develop cutting-edge software solutions.",
        applyUrl: "https://apply.workable.com/exampletech/j/F1B2C3D4E5/", // Example valid URL structure
        location: "Remote",
        source: "workable",
        matchScore: 80
      },
      {
        jobTitle: "Frontend Developer",
        company: "Web Solutions Co.",
        description: "Build responsive and user-friendly web interfaces.",
        applyUrl: "https://apply.workable.com/websolutions/j/A9B8C7D6E5/", // Example valid URL structure
        location: "New York, NY",
        source: "workable",
        matchScore: 75
      },
       {
        jobTitle: "Site Reliability Engineer",
        company: "Aptible",
        description: "Ensure the reliability and performance of our platform.",
        // Example of a real, valid Workable URL structure
        applyUrl: "https://apply.workable.com/aptible/j/6F85714800/",
        location: "Remote",
        source: "workable",
        matchScore: 70
      }
    ];

    // Filter jobs to include only those with valid Workable application URLs
    const validWorkableJobs = defaultWorkableJobs.filter(job =>
      this.isValidWorkableApplicationUrl(job.applyUrl)
    );

    console.log(`Found ${validWorkableJobs.length} valid default Workable jobs`);

    return validWorkableJobs;
  }

} // End of WorkableScraper class

// Export a singleton instance
export const workableScraper = new WorkableScraper();

// Export the getWorkableJobsForUser function wrapper for backward compatibility (if needed)
// Adjust the wrapper if the main function signature changed significantly
export const getWorkableJobsForUser = async (
  userId: number,
  progressCallback?: (progress: { current: number, total: number, status: string, jobs?: JobListing[], continueToken?: string | null }) => void,
  options: {
    pageSize?: number;
    maxInitialJobs?: number;
    searchDepth?: number;
    continueToken?: string;
    jobDetailTimeoutMs?: number; // Pass timeout option down
  } = {}
): Promise<{ jobs: JobListing[]; continueToken: string | null }> => { // Return type matches new signature
  try {
    // Call the new implementation directly
    const result = await workableScraper.getWorkableJobsForUser(
      userId,
      progressCallback,
      options
    );
    return result; // Return the object { jobs, continueToken }
  } catch (error) {
    console.error("Error in getWorkableJobsForUser wrapper:", error);
    // Decide on error handling: re-throw, or return empty state?
    return { jobs: [], continueToken: null }; // Return empty result on error
  }
};
```

Explanation of Changes:

fetchJobDetailsWithTimeout:
This new function specifically handles fetching a single job's details.
It takes a url and a timeoutMs.
It creates an AbortController and passes its signal to the fetch call.
setTimeout is used to call controller.abort() after timeoutMs.
If fetch completes before the timeout, clearTimeout prevents the abort.
If the timeout occurs, controller.abort() is called, causing the fetch promise to reject with an AbortError.
Includes basic error handling for non-OK responses and fetch errors.
Crucially: It calls your /api/workable/direct-fetch endpoint. You must ensure that endpoint itself is reasonably fast and doesn't have its own long internal timeouts that would negate this client-side timeout.
Maps the response to the WorkableJob interface.
scrapeJobsFromSearchUrl Refactored:
Extracts job links from the search results page as before.
Filters out links for jobs already processed using the state.jobIds set.
Limits the number of new details to fetch concurrently (MAX_JOBS_PER_PAGE).
Creates an array of promises (detailFetchPromises) by mapping linksToFetchDetails to calls to this.fetchJobDetailsWithTimeout.
Uses Promise.allSettled to run these fetches concurrently. This waits for all promises, regardless of success or failure.
Iterates through the results array from Promise.allSettled.
If a promise was fulfilled and returned valid details, it adds the job ID to state.jobIds, maps the WorkableJob to JobListing, and adds it to jobListings.
If a promise was rejected (e.g., timeout, network error) or fulfilled but returned null, it logs the issue but continues processing other results.
Updates state.totalJobsFound.
getWorkableJobsForUser Adjusted:
Now passes the jobDetailTimeoutMs option down to scrapeJobsFromSearchUrl.
The core pagination logic remains similar, but it now calls the refactored scrapeJobsFromSearchUrl.
The progress callback and return type now include the continueToken.
Includes logic to check state.jobIds to avoid re-processing jobs across pagination calls.
Removed extractJobInfoFromPage: Its previous role (parsing search results HTML) is now implicitly part of the link extraction in scrapeJobsFromSearchUrl, and the detail fetching is handled by fetchJobDetailsWithTimeout.
Next Steps:

Integrate: Replace your existing workable-scraper.ts content with this modified version.
Test Thoroughly: Test various scenarios:
Initial search.
Pagination ("Load More").
Searches that yield many results.
Situations where some job detail pages might be slow or unresponsive (you might need to simulate this if Workable is consistently fast).
Review /api/workable/direct-fetch: Ensure your backend endpoint that fetches the actual Workable page HTML is efficient and doesn't have overly long internal timeouts. Consider adding a timeout there as well.
Consider Background Jobs: If searches still feel too long for a synchronous API request, implement the background job queue pattern for the best user experience and robustness.
Adjust Timeouts/Concurrency: Tune the jobDetailTimeoutMs and MAX_JOBS_PER_PAGE values based on testing and observation of Workable's response times and rate limits.
This approach directly tackles the suspected root cause by making the individual, potentially slow fetches concurrent and properly cancellable, preventing one bad apple from spoiling the whole bunch.

